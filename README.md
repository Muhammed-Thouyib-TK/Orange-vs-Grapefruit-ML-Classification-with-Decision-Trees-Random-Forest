# ğŸ§º Orange vs Grapefruit: ML Classification with Decision Trees & Random Forest

An end-to-end machine learning pipeline to classify citrus fruits using physical attributes like weight, diameter, and color score. This project demonstrates the power of **decision trees**, **ensemble learning**, and **hyperparameter tuning** in achieving high classification accuracy.

---

## ğŸ“Œ Project Overview

The goal is to predict whether a fruit is an **Orange** or **Grapefruit** based on a structured dataset. This project includes all the major steps in a supervised learning workflowâ€”**data cleaning**, **exploratory data analysis**, **model building**, **hyperparameter tuning**, and **performance evaluation**.

---

## ğŸ¯ Objectives

- Preprocess citrus dataset for modeling
- Explore the structure and distribution of fruit features
- Train and compare Decision Tree vs Random Forest classifiers
- Use GridSearchCV for hyperparameter tuning
- Evaluate and visualize model performance

---

## ğŸ—‚ï¸ Dataset

Features included:

- **Diameter**  
- **Weight**  
- **Color Score**  
- **Label** (Orange/Grapefruit)

---

## ğŸ” Key Highlights of the Project

### âœ… Data Loading & Preprocessing

- Loaded the dataset with structured features  
- Checked for nulls and anomalies using `.info()` and `.describe()`  
- Applied **Label Encoding** to convert fruit types into binary numerical labels  
- Performed **80-20 train-test split** using `train_test_split()`  

### ğŸ“Š Exploratory Data Analysis (EDA)

- Visualized class distributions using:
  - Histograms
  - Boxplots
  - Pairplots
- Checked for class separability and feature influence
- Used a custom `Dis_Stats` module to extract statistical feature summaries

### ğŸ¤– Model Building

- Trained a **Decision Tree Classifier** as a baseline model  
- Improved performance with **Random Forest Classifier** â€” an ensemble method combining multiple decision trees for robustness

### ğŸ”§ Hyperparameter Tuning

- Used **GridSearchCV** to find optimal settings for Random Forest:
  - `n_estimators`
  - `max_depth`
  - `criterion`
- Reduced overfitting and improved cross-validation accuracy

### ğŸ“ˆ Model Evaluation & Comparison

Evaluated both models using:

- âœ… Accuracy Score  
- ğŸ“Š Confusion Matrix  
- ğŸ“‹ Classification Report (Precision, Recall, F1-Score)

**Result:**  
Random Forest outperformed the Decision Tree in both accuracy and generalization, showcasing the benefits of ensemble learning.

---

## ğŸ›  Tools & Technologies Used

- **Python**
  - `pandas`, `numpy` â€“ data handling
  - `matplotlib`, `seaborn` â€“ data visualization
  - `scikit-learn` â€“ ML modeling, evaluation, tuning
- **GridSearchCV** â€“ hyperparameter optimization
- **Jupyter Notebook** â€“ step-by-step workflow presentation

---

## ğŸ’¡ Conclusion

This project shows how even simple datasets can benefit from:

- Thoughtful **EDA** and feature exploration  
- Powerful **ensemble models** like Random Forest  
- Systematic **hyperparameter tuning**

In real-world classification problems, selecting the right algorithm and refining it through data-driven tuning is key to robust performance.

---

## ğŸ”® Future Enhancements

- Visualize individual decision trees in the forest  
- Add feature importance plots  
- Test on imbalanced datasets or multiclass fruit prediction  
- Deploy model using Streamlit or Flask
